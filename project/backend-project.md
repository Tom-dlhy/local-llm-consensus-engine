# Backend Project - Justifications Techniques

Ce document rÃ©capitule les choix d'architecture et de conception du backend LLM Council.

## ğŸ—ï¸ Architecture DistribuÃ©e PC1/PC2

### Choix: Master/Worker Pattern

**Justification**: L'architecture distribuÃ©e avec un **Master (PC1)** et un **Worker (PC2)** permet:
- **SÃ©paration des responsabilitÃ©s**: Le Master orchestre le workflow complet (Stage 1-2-3), le Worker se concentre sur l'infÃ©rence LLM
- **ScalabilitÃ©**: PossibilitÃ© d'ajouter plusieurs Workers si nÃ©cessaire
- **Isolement des ressources**: Les ressources GPU/CPU du Worker sont dÃ©diÃ©es aux LLMs

### RÃ´le du Chairman

Le Chairman n'est pas un service sÃ©parÃ© mais une **responsabilitÃ© du Master**. Cela simplifie l'architecture tout en respectant la contrainte "Chairman sur PC #1".

```
PC 1 (Master)                    PC 2 (Worker)
â”œâ”€â”€ Orchestration                â”œâ”€â”€ Ollama API
â”œâ”€â”€ Stage 1: Dispatch agents     â”œâ”€â”€ /api/generate
â”œâ”€â”€ Stage 2: Dispatch reviews    â””â”€â”€ Multi-model parallel
â””â”€â”€ Stage 3: Chairman synthesis
```

## ğŸ“¦ Stack Technique

| Composant | Choix | Justification |
|-----------|-------|---------------|
| **Framework API** | FastAPI | Async natif, WebSocket support, OpenAPI auto-gÃ©nÃ©rÃ© |
| **Configuration** | Pydantic Settings | Validation forte, env vars, type safety |
| **HTTP Client** | httpx | Async, timeouts configurables, streaming |
| **Monitoring** | psutil | LÃ©ger, cross-platform, CPU/RAM metrics |

## â±ï¸ Gestion des Timeouts

**ProblÃ¨me**: Les LLMs peuvent prendre 30-60s par rÃ©ponse, surtout avec 5 modÃ¨les en parallÃ¨le.

**Solution**: 
- Timeout par dÃ©faut: **120 secondes**
- Connect timeout: **10 secondes** (dÃ©tection rapide des erreurs rÃ©seau)
- Configurable via `GENERATION_TIMEOUT`

```python
httpx.Timeout(120.0, connect=10.0)
```

## ğŸ”„ ParallÃ©lisme et Concurrence

### Configuration Ollama (PC2)
```bash
export OLLAMA_NUM_PARALLEL=5       # 5 requÃªtes simultanÃ©es
export OLLAMA_MAX_LOADED_MODELS=5  # 5 modÃ¨les en mÃ©moire
```

### Code Async
```python
# Stage 1: Toutes les opinions en parallÃ¨le
responses = await asyncio.gather(*[generate(agent) for agent in agents])
```

**Justification**: `asyncio.gather` permet de lancer les N requÃªtes instantanÃ©ment. Le Worker traite en parallÃ¨le (limitÃ© par OLLAMA_NUM_PARALLEL).

## ğŸ†” Gestion des IDs Uniques

**ProblÃ¨me**: L'utilisateur peut sÃ©lectionner 2x le mÃªme modÃ¨le (ex: 2 instances de `llama3.2:1b`).

**Solution**: Chaque agent reÃ§oit un ID unique (`agent_1`, `agent_2`) indÃ©pendamment du modÃ¨le:

```python
for i, agent in enumerate(request.selected_agents):
    agent_id = f"agent_{i + 1}"
```

Cela permet au Chairman de distinguer les contributions lors de la synthÃ¨se.

## ğŸ“‹ JSON Mode (Stage 2)

**ProblÃ¨me**: Le Stage 2 (Review) nÃ©cessite un format structurÃ© pour extraire les scores.

**Solution**: Utilisation du paramÃ¨tre `format: "json"` d'Ollama:

```python
response = await ollama.generate(
    model=model,
    prompt=review_prompt,
    format="json"  # Force JSON output
)
```

**Prompt structurÃ©**:
```
Respond ONLY with valid JSON in the following format:
{
    "rankings": [
        {"agent_id": "<id>", "score": <1-10>, "reasoning": "<explanation>"}
    ]
}
```

## ğŸ“¡ API Design

### Worker (`/api/generate`)
- Endpoint simple et stateless
- Compatible avec le workflow Master
- Batch endpoint pour optimisation future

### Master (`/api/council/*`)
- Session-based (UUID tracking)
- WebSocket pour streaming
- Liste des modÃ¨les recommandÃ©s

### Health Checks
- `/health`: Statut du service
- `/health/system`: CPU/RAM usage
- `/health/ollama`: Connexion Ollama
- `/health/models`: ModÃ¨les disponibles

## ğŸ¯ ModÃ¨les RecommandÃ©s

| ModÃ¨le | Taille | RÃ´le Optimal |
|--------|--------|--------------|
| `qwen2.5:0.5b` | 350 MB | Opinions rapides (Stage 1) |
| `llama3.2:1b` | 1.3 GB | Review/notation (Stage 2) |
| `gemma2:2b` | 1.6 GB | Expert prÃ©cis |
| `phi3.5:mini` | 2.2 GB | Chairman (Stage 3) |
| `tinyllama` | 600 MB | Backup lÃ©ger |

**Note**: GPT-2 exclu car incapable de produire du JSON structurÃ© fiable.

## ğŸ” SÃ©curitÃ©

- CORS configurÃ© (Ã  restreindre en production)
- Pas d'authentification (rÃ©seau local assumÃ©)
- Validation Pydantic sur tous les inputs

## ğŸ“ˆ Ã‰volutions Futures

1. **Caching**: Mise en cache des rÃ©ponses identiques
2. **Queue System**: Redis/RabbitMQ pour gÃ©rer la charge
3. **Multi-Worker**: Load balancing entre plusieurs PC2
4. **Metrics**: Prometheus/Grafana pour monitoring avancÃ©
