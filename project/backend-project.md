# Backend Project - Justifications Techniques

Ce document r√©capitule les choix d'architecture et de conception du backend LLM Council.

## üèóÔ∏è Architecture Distribu√©e PC1/PC2

### Choix: Master/Worker Pattern

**Justification**: L'architecture distribu√©e avec un **Master (PC1)** et un **Worker (PC2)** permet:
- **S√©paration des responsabilit√©s**: Le Master orchestre le workflow complet (Stage 1-2-3), le Worker se concentre sur l'inf√©rence LLM
- **Scalabilit√©**: Possibilit√© d'ajouter plusieurs Workers si n√©cessaire
- **Isolement des ressources**: Les ressources GPU/CPU du Worker sont d√©di√©es aux LLMs

### R√¥le du Chairman

Le Chairman n'est pas un service s√©par√© mais une **responsabilit√© du Master**. Cela simplifie l'architecture tout en respectant la contrainte "Chairman sur PC #1".

```
PC 1 (Master)                    PC 2 (Worker)
‚îú‚îÄ‚îÄ Orchestration                ‚îú‚îÄ‚îÄ Ollama API
‚îú‚îÄ‚îÄ Stage 1: Dispatch agents     ‚îú‚îÄ‚îÄ /api/generate
‚îú‚îÄ‚îÄ Stage 2: Dispatch reviews    ‚îî‚îÄ‚îÄ Multi-model parallel
‚îî‚îÄ‚îÄ Stage 3: Chairman synthesis
```

## üì¶ Stack Technique

| Composant | Choix | Justification |
|-----------|-------|---------------|
| **Framework API** | FastAPI | Async natif, WebSocket support, OpenAPI auto-g√©n√©r√© |
| **Configuration** | Pydantic Settings | Validation forte, env vars, type safety |
| **HTTP Client** | httpx | Async, timeouts configurables, streaming |
| **Monitoring** | psutil | L√©ger, cross-platform, CPU/RAM metrics |

## ‚è±Ô∏è Gestion des Timeouts

**Probl√®me**: Les LLMs peuvent prendre 30-60s par r√©ponse, surtout avec 5 mod√®les en parall√®le.

**Solution**: 
- Timeout par d√©faut: **120 secondes**
- Connect timeout: **10 secondes** (d√©tection rapide des erreurs r√©seau)
- Configurable via `GENERATION_TIMEOUT`

```python
httpx.Timeout(120.0, connect=10.0)
```

## üîÑ Parall√©lisme et Concurrence

### Configuration Ollama (PC2)
```bash
export OLLAMA_NUM_PARALLEL=5       # 5 requ√™tes simultan√©es
export OLLAMA_MAX_LOADED_MODELS=5  # 5 mod√®les en m√©moire
```

### Code Async
```python
# Stage 1: Toutes les opinions en parall√®le
responses = await asyncio.gather(*[generate(agent) for agent in agents])
```

**Justification**: `asyncio.gather` permet de lancer les N requ√™tes instantan√©ment. Le Worker traite en parall√®le (limit√© par OLLAMA_NUM_PARALLEL).

## üÜî Gestion des IDs Uniques

**Probl√®me**: L'utilisateur peut s√©lectionner 2x le m√™me mod√®le (ex: 2 instances de `llama3.2:1b`).

**Solution**: Chaque agent re√ßoit un ID unique (`agent_1`, `agent_2`) ind√©pendamment du mod√®le:

```python
for i, agent in enumerate(request.selected_agents):
    agent_id = f"agent_{i + 1}"
```

Cela permet au Chairman de distinguer les contributions lors de la synth√®se.

## üìã JSON Mode (Stage 2)

**Probl√®me**: Le Stage 2 (Review) n√©cessite un format structur√© pour extraire les scores.

**Solution**: Utilisation du param√®tre `format: "json"` d'Ollama:

```python
response = await ollama.generate(
    model=model,
    prompt=review_prompt,
    format="json"  # Force JSON output
)
```

**Prompt structur√©**:
```
Respond ONLY with valid JSON in the following format:
{
    "rankings": [
        {"agent_id": "<id>", "score": <1-10>, "reasoning": "<explanation>"}
    ]
}
```

## üì° API Design

### Worker (`/api/generate`)
- Endpoint simple et stateless
- Compatible avec le workflow Master
- Batch endpoint pour optimisation future

### Master (`/api/council/*`)
- Session-based (UUID tracking)
- WebSocket pour streaming
- Liste des mod√®les recommand√©s

### Health Checks
- `/health`: Statut du service
- `/health/system`: CPU/RAM usage
- `/health/ollama`: Connexion Ollama
- `/health/models`: Mod√®les disponibles

## üìä Token Usage Estimation

### Objectif
Suivi complet de la consommation de tokens √† travers les 3 phases du workflow, permettant l'affichage de m√©triques d√©taill√©es dans le frontend.

### Mod√®les de Donn√©es

```python
class TokenUsage(BaseModel):
    """Usage pour une g√©n√©ration individuelle."""
    prompt_tokens: int      # Tokens d'entr√©e (prompt_eval_count)
    completion_tokens: int  # Tokens g√©n√©r√©s (eval_count)
    total_tokens: int       # Total

class StageTokenUsage(BaseModel):
    """Usage agr√©g√© par phase."""
    stage: str                           # "opinions", "review", "synthesis"
    total_prompt_tokens: int
    total_completion_tokens: int
    total_tokens: int
    by_model: dict[str, TokenUsage]      # Breakdown par mod√®le

class SessionTokenUsage(BaseModel):
    """Usage complet de la session."""
    stage1_opinions: StageTokenUsage | None
    stage2_review: StageTokenUsage | None
    stage3_synthesis: StageTokenUsage | None
    total_prompt_tokens: int
    total_completion_tokens: int
    total_tokens: int

class SessionLatencyStats(BaseModel):
    """Latence compl√®te de la session (KPI)."""
    stage1_opinions: StageLatencyStats | None
    stage2_review: StageLatencyStats | None
    stage3_synthesis: StageLatencyStats | None
    total_duration_ms: int
```

### R√©ponse API

Le champ `token_usage` est maintenant inclus dans `CouncilSession`:

```json
{
  "token_usage": {
    "stage1_opinions": {
      "stage": "opinions",
      "total_prompt_tokens": 244,
      "total_completion_tokens": 256,
      "total_tokens": 500,
      "by_model": {
        "gemma2:2b": {"prompt_tokens": 82, "completion_tokens": 61, "total_tokens": 143},
        "qwen2.5:0.5b": {"prompt_tokens": 81, "completion_tokens": 96, "total_tokens": 177}
      }
    },
    "stage2_review": { ... },
    "stage3_synthesis": { ... },
    "total_prompt_tokens": 2038,
    "total_completion_tokens": 1202,
    "total_tokens": 3240
  }
}
```

### M√©triques de Test (3 agents)

| Stage | Prompt | Completion | Total |
|-------|--------|------------|-------|
| Stage 1 (Opinions) | 244 | 256 | 500 |
| Stage 2 (Review) | 1,293 | 594 | 1,887 |
| Stage 3 (Synthesis) | 501 | 352 | 853 |
| **TOTAL** | **2,038** | **1,202** | **3,240** |

### Impl√©mentation

- `_generate_opinion()`: Capture `prompt_eval_count` et `eval_count` d'Ollama
- `_generate_review()`: Idem pour les reviews
- `stage3_synthesis()`: Idem pour le Chairman
- `_calculate_stage_usage()`: Agr√®ge par mod√®le
- `_update_total_usage()`: Calcule les totaux globaux

## üéØ Mod√®les Recommand√©s

| Mod√®le | Taille | R√¥le Optimal |
|--------|--------|--------------|
| `qwen2.5:0.5b` | 350 MB | Opinions rapides (Stage 1) |
| `llama3.2:1b` | 1.3 GB | Review/notation (Stage 2) |
| `gemma2:2b` | 1.6 GB | Expert pr√©cis |
| `phi3.5:mini` | 2.2 GB | Chairman (Stage 3) |
| `tinyllama` | 600 MB | Backup l√©ger |

**Note**: GPT-2 exclu car incapable de produire du JSON structur√© fiable.

## üîê S√©curit√©

- CORS configur√© (√† restreindre en production)
- Pas d'authentification (r√©seau local assum√©)
- Validation Pydantic sur tous les inputs

## üìà √âvolutions Futures

1. **Caching**: Mise en cache des r√©ponses identiques
2. **Queue System**: Redis/RabbitMQ pour g√©rer la charge
3. **Multi-Worker**: Load balancing entre plusieurs PC2
4. **Metrics**: Prometheus/Grafana pour monitoring avanc√©
5. **Cost Estimation**: Estimation du co√ªt √©quivalent API cloud bas√©e sur les tokens
6. **Detailed Tracing**: OpenTelemetry tracing pour voir la latence de chaque span (r√©seau vs LLM)

