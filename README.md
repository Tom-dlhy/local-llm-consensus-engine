# ğŸ§  Local LLM Consensus Engine

Un systÃ¨me de consensus distribuÃ© utilisant plusieurs LLMs locaux via Ollama. Les agents dÃ©libÃ¨rent, se notent mutuellement, et un Chairman synthÃ©tise la rÃ©ponse finale.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WORKFLOW                                 â”‚
â”‚  Stage 1: Opinions  â†’  Stage 2: Reviews  â†’  Stage 3: Synthesis  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Mode Solo (1 PC)                    Mode DistribuÃ© (2 PCs)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Master + Worker     â”‚            â”‚ PC1 (Master)â”‚â”€â”€â”€â–¶â”‚ PC2 (Worker)â”‚
â”‚ localhost:8000/8001 â”‚            â”‚ Chairman    â”‚    â”‚ Ollama LLMs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ PrÃ©requis

- **Python 3.12+**
- **[uv](https://docs.astral.sh/uv/)** - Gestionnaire de paquets Python
- **[Ollama](https://ollama.ai/)** - Runtime LLM local
- **Node.js 18+** (pour le frontend)

---

## ğŸ¤– Installation des ModÃ¨les Ollama

### ModÃ¨les essentiels

```bash
ollama pull qwen2.5:0.5b    # Opinions rapides (350 MB)
ollama pull llama3.2:1b     # Review/notation (1.3 GB)
ollama pull phi3.5:latest   # Chairman (2.2 GB)
```

### ModÃ¨les optionnels

```bash
ollama pull gemma2:2b       # Expert prÃ©cis (1.6 GB)
ollama pull tinyllama       # Backup lÃ©ger (600 MB)
```

---

## ğŸš€ Lancement du Projet

### Option 1 : Mode Solo (DÃ©veloppement)

Tout sur une seule machine avec deux terminaux.

#### Terminal 1 - Worker (InfÃ©rence LLM)

```bash
# Configuration Ollama pour le parallÃ©lisme
export OLLAMA_NUM_PARALLEL=5
export OLLAMA_MAX_LOADED_MODELS=5

# Lancer Ollama
ollama serve
```

#### Terminal 2 - Backend Worker

```bash
cd backend
uv sync
uv run python -m src.main --role worker --port 8001
```

#### Terminal 3 - Backend Master

```bash
cd backend
uv run python -m src.main --role master --worker-url http://localhost:8001
```

#### Terminal 4 - Frontend

```bash
cd frontend
npm install
npm run dev
```

**AccÃ¨s :**
- ğŸŒ Frontend : http://localhost:5173
- ğŸ“¡ API Master : http://localhost:8000
- ğŸ“š API Docs : http://localhost:8000/docs

---

### Option 2 : Mode DistribuÃ© (2 PCs)

Architecture optimale avec sÃ©paration des ressources.

#### ğŸ–¥ï¸ PC 2 - Worker (Machine avec GPU/ressources LLM)

```bash
# 1. Configuration Ollama
export OLLAMA_NUM_PARALLEL=5
export OLLAMA_MAX_LOADED_MODELS=5
ollama serve

# 2. Lancer le Worker (nouveau terminal)
cd backend
uv sync
uv run python -m src.main --role worker --host 0.0.0.0 --port 8000
```

> **Note :** `--host 0.0.0.0` permet les connexions depuis le rÃ©seau local.

#### ğŸ–¥ï¸ PC 1 - Master (Orchestration + Chairman)

```bash
# 1. Lancer le Master (remplacer IP_DU_PC2)
cd backend
uv sync
uv run python -m src.main --role master --worker-url http://IP_DU_PC2:8000

# 2. Lancer le Frontend (nouveau terminal)
cd frontend
npm install
npm run dev
```

**Exemple avec IP :**
```bash
uv run python -m src.main --role master --worker-url http://192.168.1.42:8000
```

---

## âš™ï¸ Variables d'Environnement

CrÃ©ez un fichier `.env` dans le dossier `backend/` :

```env
# RÃ´le du serveur
ROLE=master  # ou "worker"

# Configuration rÃ©seau
HOST=0.0.0.0
PORT=8000

# Ollama
OLLAMA_BASE_URL=http://localhost:11434

# Master only
WORKER_URL=http://localhost:8001
CHAIRMAN_MODEL=phi3.5:latest

# Timeouts (secondes)
GENERATION_TIMEOUT=120
```

---

## ğŸ“¡ VÃ©rification du Setup

### Tester la connexion Ollama

```bash
curl http://localhost:11434/api/tags
```

### Tester le Worker

```bash
curl http://localhost:8001/health
curl http://localhost:8001/health/models
```

### Tester le Master

```bash
curl http://localhost:8000/health
curl http://localhost:8000/api/council/models
```

---

## ğŸ§ª Exemple de RequÃªte API

```bash
curl -X POST http://localhost:8000/api/council/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Quelle est la meilleure approche pour apprendre la programmation ?",
    "selected_agents": [
      {"name": "Expert_1", "model": "qwen2.5:0.5b"},
      {"name": "Expert_2", "model": "llama3.2:1b"},
      {"name": "Expert_3", "model": "gemma2:2b"}
    ],
    "chairman_model": "phi3.5:latest"
  }'
```

---

## ğŸ“‚ Structure du Projet

```
local-llm-consensus-engine/
â”œâ”€â”€ backend/                 # API FastAPI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.py         # Point d'entrÃ©e CLI
â”‚   â”‚   â”œâ”€â”€ config.py       # Configuration Pydantic
â”‚   â”‚   â”œâ”€â”€ models/         # ModÃ¨les de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ services/       # Logique mÃ©tier (Council, Ollama)
â”‚   â”‚   â””â”€â”€ api/            # Routes FastAPI
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ frontend/                # Interface React/Vite
â””â”€â”€ project/                 # Documentation technique
```

---

## ğŸ”§ DÃ©pannage

| ProblÃ¨me | Solution |
|----------|----------|
| `Connection refused` sur Worker | VÃ©rifiez que Ollama tourne (`ollama serve`) |
| Timeout sur gÃ©nÃ©ration | Augmentez `GENERATION_TIMEOUT` ou utilisez des modÃ¨les plus lÃ©gers |
| ModÃ¨le non trouvÃ© | ExÃ©cutez `ollama pull <model>` |
| CORS error | Le Master doit tourner sur le port attendu par le frontend |

---

## ğŸ“š Documentation

- **API Swagger** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc
- **Architecture dÃ©taillÃ©e** : [project/backend-project.md](project/backend-project.md)
