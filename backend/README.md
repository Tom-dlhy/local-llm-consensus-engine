# LLM Council Backend

Backend distribuÃ© pour le **LLM Council** - Un systÃ¨me de consensus local utilisant plusieurs LLMs.

## ğŸ—ï¸ Architecture

Le systÃ¨me utilise une architecture **Master/Worker** distribuÃ©e :

```
PC 1 (Master)                    PC 2 (Worker)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI :8000       â”‚         â”‚ FastAPI :8000       â”‚
â”‚ â”œâ”€ /api/council/*   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â”œâ”€ /api/generate    â”‚
â”‚ â””â”€ Chairman         â”‚         â”‚ â””â”€ Ollama           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### PrÃ©requis
- Python 3.12+
- [uv](https://docs.astral.sh/uv/) (gestionnaire de paquets)
- [Ollama](https://ollama.ai/) installÃ© et en cours d'exÃ©cution

### Installation des dÃ©pendances
```bash
cd backend
uv sync
```

### Configuration Ollama (PC 2)
```bash
# Permettre le parallÃ©lisme
export OLLAMA_NUM_PARALLEL=5
export OLLAMA_MAX_LOADED_MODELS=5
ollama serve
```

### ModÃ¨les recommandÃ©s
```bash
# TÃ©lÃ©charger les modÃ¨les
ollama pull qwen2.5:0.5b    # Opinions rapides
ollama pull llama3.2:1b     # Review/notation
ollama pull phi3.5:latest     # Chairman
```

## ğŸš€ DÃ©marrage

### Mode Worker (PC 2 - Inference LLM)
```bash
cd backend
uv run python -m src.main --role worker
```

### Mode Master (PC 1 - Orchestration)
```bash
cd backend
uv run python -m src.main --role master --worker-url http://PC2_IP:8000
```

### Mode dÃ©veloppement (tout-en-un)
```bash
# Terminal 1: Worker
uv run python -m src.main --role worker --port 8001

# Terminal 2: Master
uv run python -m src.main --role master --worker-url http://localhost:8001
```

## ğŸ“¡ API Endpoints

### Health Check
- `GET /health` - Statut du service
- `GET /health/ollama` - Connexion Ollama
- `GET /health/system` - CPU/RAM
- `GET /health/models` - ModÃ¨les disponibles

### Worker (PC 2)
- `POST /api/generate` - GÃ©nÃ©ration LLM simple
- `POST /api/generate/batch` - GÃ©nÃ©ration parallÃ¨le

### Master (PC 1)
- `POST /api/council/query` - Lancer une dÃ©libÃ©ration
- `GET /api/council/session/{id}` - Ã‰tat d'une session
- `GET /api/council/models` - ModÃ¨les recommandÃ©s
- `WebSocket /api/council/ws/{id}` - Streaming temps rÃ©el

## ğŸ“ Exemple de requÃªte

```bash
curl -X POST http://localhost:8000/api/council/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Quelle est la capitale de la France ?",
    "selected_agents": [
      {"name": "Expert_1", "model": "qwen2.5:0.5b"},
      {"name": "Expert_2", "model": "llama3.2:1b"},
      {"name": "Expert_3", "model": "gemma2:2b"}
    ],
    "chairman_model": "phi3.5:latest"
  }'
```

## ğŸ”§ Variables d'environnement

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `ROLE` | `worker` | `master` ou `worker` |
| `HOST` | `0.0.0.0` | Adresse de bind |
| `PORT` | `8000` | Port d'Ã©coute |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | URL Ollama |
| `WORKER_URL` | `http://localhost:8000` | URL du Worker (Master) |
| `CHAIRMAN_MODEL` | `phi3.5:latest` | ModÃ¨le Chairman |
| `GENERATION_TIMEOUT` | `120` | Timeout gÃ©nÃ©ration (s) |

## ğŸ“‚ Structure

```
src/
â”œâ”€â”€ main.py           # Point d'entrÃ©e CLI
â”œâ”€â”€ config.py         # Configuration Pydantic
â”œâ”€â”€ models/           # ModÃ¨les de donnÃ©es
â”‚   â””â”€â”€ council.py    # AgentConfig, CouncilSession, etc.
â”œâ”€â”€ services/         # Logique mÃ©tier
â”‚   â”œâ”€â”€ ollama_client.py  # Client HTTP Ollama
â”‚   â””â”€â”€ council.py    # Orchestration Stage 1-2-3
â””â”€â”€ api/              # Routes FastAPI
    â”œâ”€â”€ council_routes.py  # Master endpoints
    â”œâ”€â”€ worker_routes.py   # Worker endpoints
    â””â”€â”€ health_routes.py   # Monitoring
```

## ğŸ“š Documentation API

AprÃ¨s dÃ©marrage, accÃ©dez Ã  :
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
